{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 4271 - Exercise 4 - Statistical Ranking\n",
    "\n",
    "Issued: May 7, 2024\n",
    "\n",
    "Due: May 13, 2024\n",
    "\n",
    "Please submit this filled sheet via Ilias by the due date.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Generative Relevance Models\n",
    "Generative retrieval models use the probabilistic language model framework for matching queries and documents.\n",
    "\n",
    "a) Implement the `rank()` function sketched below. In class, we discussed two alternative model variants. Choose the query likelihood model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. the french revolution was a period of upheaval in france\n",
      "2. the french bulldog is a small breed of domestic dog\n",
      "3. french is a very french language spoken by the french\n"
     ]
    }
   ],
   "source": [
    "#Rank a collection of documents relative to a query using the query likelihood model\n",
    "def rank(Q, D):\n",
    "     query_terms = Q.lower().split()\n",
    "     document_terms = [doc.lower().split() for doc in D]\n",
    "\n",
    "     doc_language_models = []\n",
    "     for doc_terms in document_terms:\n",
    "          doc_model = {}\n",
    "          for term in doc_terms:\n",
    "               doc_model[term] = doc_terms.count(term) / len(doc_terms)\n",
    "          doc_language_models.append(doc_model)\n",
    " \n",
    "     ranked_documents = []\n",
    "     for i, doc_model in enumerate(doc_language_models):\n",
    "          query_likelihood = 1.0\n",
    "          for term in query_terms:\n",
    "               if term in doc_model:\n",
    "                    query_likelihood *= doc_model[term]\n",
    "               else:\n",
    "                    # Smoothing\n",
    "                    query_likelihood *= 1e-6\n",
    "          ranked_documents.append((i, query_likelihood))\n",
    "          \n",
    "     return ranked_documents\n",
    "\n",
    "Q = 'french bulldog'\n",
    "D = ['the french revolution was a period of upheaval in france', \n",
    "     'the french bulldog is a small breed of domestic dog', \n",
    "     'french is a very french language spoken by the french']\n",
    "\n",
    "print(rank(Q, D))                            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Probabilistic language models may encounter previously unseen query terms. Explain why this can become problematic and how you would address the issue. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a probabilistic language model encounters an unseen query term in the query, the probability of the whole query will automatically become 0. To prevent this, we can do some smoothing. This means adding a small constant to all terms to avoid 0 probabilities."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Relevance Feedback\n",
    "Relevance Feedback allows us to refine the query representation after a round of user interaction with the search results. If organic feedback is not available, we can assume highly ranked documents to be *pseudo* relevant. Discuss the advantages and disadvantages of the pseudo relevance feedback scheme. Think in particular about single versus multiple rounds of feedback."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disadvantages:\n",
    "-   Topic drift: By selecting the nearest relevant topics and also querying them, we risk that the topics of the documents found don't always match the original topic of the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! Let's delve into the concept of **pseudo relevance feedback** and explore its advantages and disadvantages.\n",
    "\n",
    "## Pseudo Relevance Feedback (PRF)\n",
    "Pseudo relevance feedback is a technique used in information retrieval systems to improve retrieval performance without requiring extensive user interaction. Here's how it works:\n",
    "\n",
    "1. **Initial Retrieval**:\n",
    "   - The system performs an initial retrieval using the user's query to find a set of top-ranked documents.\n",
    "   - These top-ranked documents are assumed to be relevant, even though they haven't been explicitly labeled as such by the user.\n",
    "\n",
    "2. **Query Modification**:\n",
    "   - Based on the assumption that the top-ranked documents contain relevant information, the system modifies the original query.\n",
    "   - The modified query aims to capture the underlying concepts present in the relevant documents.\n",
    "\n",
    "3. **Relevance Feedback**:\n",
    "   - The modified query is then used for a subsequent retrieval.\n",
    "   - The user may provide feedback on the relevance of the results obtained with the modified query.\n",
    "   - This feedback can further refine the query representation.\n",
    "\n",
    "### Advantages of Pseudo Relevance Feedback:\n",
    "1. **Automatic and Efficient**:\n",
    "   - PRF automates the manual part of relevance feedback, reducing the need for extensive user interaction.\n",
    "   - Users benefit from improved retrieval performance without spending additional time providing explicit feedback.\n",
    "\n",
    "2. **Better Performance than Global Analysis**:\n",
    "   - Evidence suggests that PRF tends to work better than global analysis methods.\n",
    "   - By focusing on the top-ranked documents, PRF captures relevant signals more effectively.\n",
    "\n",
    "3. **Effective in TREC Ad Hoc Tasks**:\n",
    "   - PRF has been successfully applied in the Text Retrieval Conference (TREC) ad hoc task, where it improved retrieval effectiveness.\n",
    "\n",
    "### Disadvantages of Pseudo Relevance Feedback:\n",
    "1. **Assumption-Based Risk**:\n",
    "   - The assumption that top-ranked documents are relevant may not always hold true.\n",
    "   - If the initial retrieval retrieves irrelevant documents due to noise or query drift, the modified query may also be suboptimal.\n",
    "\n",
    "2. **Query Drift**:\n",
    "   - Query drift occurs when the modified query shifts toward the content of the top-ranked documents, even if they are not truly relevant.\n",
    "   - For example, if the initial query is about \"copper mines,\" but the top-ranked documents are all about \"mines in Chile,\" the modified query may drift toward Chile-related content.\n",
    "\n",
    "3. **Single vs. Multiple Rounds of Feedback**:\n",
    "   - PRF typically involves a single round of feedback. However, multiple rounds can be performed.\n",
    "   - Single-round PRF may not fully capture user intent, while multiple rounds can lead to overfitting or excessive drift.\n",
    "\n",
    "In summary, pseudo relevance feedback strikes a balance between automation and risk. While it offers efficiency and improved performance, careful consideration of assumptions and potential query drift is essential. Researchers continue to explore ways to enhance PRF and mitigate its limitations¹². If you have any further questions or need additional clarification, feel free to ask!\n",
    "\n",
    "Quelle: Unterhaltung mit Bing, 11.5.2024\n",
    "(1) Pseudo relevance feedback - Stanford University. https://nlp.stanford.edu/IR-book/html/htmledition/pseudo-relevance-feedback-1.html.\n",
    "(2) Relevance feedback and pseudo relevance feedback - Stanford University. https://nlp.stanford.edu/IR-book/html/htmledition/relevance-feedback-and-pseudo-relevance-feedback-1.html.\n",
    "(3) Pseudo Relevance Feedback with Deep Language Models and Dense .... https://arxiv.org/pdf/2108.11044v1.\n",
    "(4) Pseudo Relevance Feedback with Deep Language Models and Dense .... https://arxiv.org/pdf/2108.11044v2.pdf."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
