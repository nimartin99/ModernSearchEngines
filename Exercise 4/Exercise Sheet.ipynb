{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 4271 - Exercise 4 - Statistical Ranking\n",
    "\n",
    "Issued: May 7, 2024\n",
    "\n",
    "Due: May 13, 2024\n",
    "\n",
    "Please submit this filled sheet via Ilias by the due date.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Generative Relevance Models\n",
    "Generative retrieval models use the probabilistic language model framework for matching queries and documents.\n",
    "\n",
    "a) Implement the `rank()` function sketched below. In class, we discussed two alternative model variants. Choose the query likelihood model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1e-07), (1, 0.010000000000000002), (2, 3e-07)]\n"
     ]
    }
   ],
   "source": [
    "#Rank a collection of documents relative to a query using the query likelihood model\n",
    "def rank(Q, D):\n",
    "     query_terms = Q.lower().split()\n",
    "     document_terms = [doc.lower().split() for doc in D]\n",
    "\n",
    "     doc_language_models = []\n",
    "     for doc_terms in document_terms:\n",
    "          doc_model = {}\n",
    "          for term in doc_terms:\n",
    "               doc_model[term] = doc_terms.count(term) / len(doc_terms)\n",
    "          doc_language_models.append(doc_model)\n",
    " \n",
    "     ranked_documents = []\n",
    "     for i, doc_model in enumerate(doc_language_models):\n",
    "          query_likelihood = 1.0\n",
    "          for term in query_terms:\n",
    "               if term in doc_model:\n",
    "                    query_likelihood *= doc_model[term]\n",
    "               else:\n",
    "                    # Smoothing\n",
    "                    query_likelihood *= 1e-6\n",
    "          ranked_documents.append((i, query_likelihood))\n",
    "          \n",
    "     return ranked_documents\n",
    "\n",
    "Q = 'french bulldog'\n",
    "D = ['the french revolution was a period of upheaval in france', \n",
    "     'the french bulldog is a small breed of domestic dog', \n",
    "     'french is a very french language spoken by the french']\n",
    "\n",
    "print(rank(Q, D))                            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Probabilistic language models may encounter previously unseen query terms. Explain why this can become problematic and how you would address the issue. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a probabilistic language model encounters an unseen query term in the query, the probability of the whole query will automatically become 0. To prevent this, we can do some smoothing. This means adding a small constant to all terms to avoid 0 probabilities."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Relevance Feedback\n",
    "Relevance Feedback allows us to refine the query representation after a round of user interaction with the search results. If organic feedback is not available, we can assume highly ranked documents to be *pseudo* relevant. Discuss the advantages and disadvantages of the pseudo relevance feedback scheme. Think in particular about single versus multiple rounds of feedback."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "- The chance of getting more relevant documents gets increased. If we use multiple rounds of feedback, the chance of getting even closer to the original query increases if the query was formed well.\n",
    "- We get better results even if the query of the user isn't well formed.\n",
    "- When forming a new query with the pseudo relevant feedback we can detect underlying concepts that weren't clear through the original query but are revealed by the pseudo relevant feedback documents.\n",
    "- We don't need user interaction to get feedback (users never tend to give feedback). This saves time and interaction actions by the user.\n",
    "\n",
    "Disadvantages:\n",
    "- Topic drift: By selecting the nearest relevant topics and also querying them, we risk that the topics of the documents found don't always match the original topic of the query. With multiple rounds of feedback we increase the liklyhood of having non-relevant feedback even further.\n",
    "- If the inital query results in irrelevant documents, the second query based on pseudo relevance feedback topics can be far off what the user actually wanted."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
