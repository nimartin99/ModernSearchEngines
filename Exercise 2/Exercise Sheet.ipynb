{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 4271 - Exercise 2 - Text Representation\n",
    "\n",
    "Issued: April 23, 2024\n",
    "\n",
    "Due: April 29, 2024\n",
    "\n",
    "Please submit this filled sheet via Ilias by the due date.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Bag-of-Words Models\n",
    "In class we discussed BOW vectorization models under which documents are represented via term frequency counts.\n",
    "\n",
    "a) Construct term frequency BOW representations for the following sentences:\n",
    "\n",
    "- \"The government is open.\"\n",
    "- \"The government is closed.\"\n",
    "- \"Long live Mickey Mouse, emperor of all!\"\n",
    "- \"Darn! This will break.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'the': 1, 'government': 1, 'is': 1, 'open': 1}, {'the': 1, 'government': 1, 'is': 1, 'closed': 1}, {'long': 1, 'live': 1, 'mickey': 1, 'mouse': 1, 'emperor': 1, 'of': 1, 'all': 1}, {'darn': 1, 'this': 1, 'will': 1, 'break': 1}]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "corpus = [['The government is open.'], ['The government is closed.'], ['Long live Mickey Mouse, emperor of all!'], ['Darn! This will break.']]\n",
    "\n",
    "#Turn a corpus of arbitrary texts into term-frequency weighted BOW vectors.\n",
    "def TF(corpus):\n",
    "    vecs = []\n",
    "    #TODO: Implement me!\n",
    "    for text in corpus:\n",
    "        text[0] = text[0].translate(str.maketrans('', '', string.punctuation))\n",
    "        text[0] = text[0].lower()\n",
    "        words = text[0].split()\n",
    "        bow = {}\n",
    "        for word in words:\n",
    "            if word in bow:\n",
    "                bow[word] += 1\n",
    "            else:\n",
    "                bow[word] = 1\n",
    "        vecs.append(bow)\n",
    "    return vecs\n",
    "\n",
    "print(TF(corpus))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Extend the term frequency model by an inverse document frequency (IDF) component. Estimate IDFs based on the Reuters 21578 collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\nic0m\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7769 total train documents\n",
      "The first document contains 633 words.\n",
      "Here they are:\n",
      "['BAHIA', 'COCOA', 'REVIEW', 'Showers', 'continued', ...]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 87\u001b[0m\n\u001b[0;32m     84\u001b[0m             bow[word] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bow\n\u001b[1;32m---> 87\u001b[0m \u001b[43mTFIDF\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 73\u001b[0m, in \u001b[0;36mTFIDF\u001b[1;34m(corpus)\u001b[0m\n\u001b[0;32m     71\u001b[0m         word_fwbogs \u001b[38;5;241m=\u001b[39m fwbogs[i]\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;28mprint\u001b[39m(word_fwbogs)\n\u001b[1;32m---> 73\u001b[0m         tf_idf_vec[word] \u001b[38;5;241m=\u001b[39m \u001b[43mword_fwbogs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mlog10(\u001b[38;5;28mlen\u001b[39m(corpus) \u001b[38;5;241m/\u001b[39m document_count)\n\u001b[0;32m     74\u001b[0m     vecs\u001b[38;5;241m.\u001b[39mappend(tf_idf_vec)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(vecs)\n",
      "File \u001b[1;32mc:\\Users\\nic0m\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\reader\\util.py:255\u001b[0m, in \u001b[0;36mStreamBackedCorpusView.__getitem__\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LazySubsequence(\u001b[38;5;28mself\u001b[39m, start, stop)\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;66;03m# Handle negative indices\u001b[39;00m\n\u001b[1;32m--> 255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m:\n\u001b[0;32m    256\u001b[0m         i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "import math\n",
    "\n",
    "#Download the documents\n",
    "nltk.download(\"reuters\")\n",
    "documents = reuters.fileids()\n",
    "\n",
    "docs = list(filter(lambda doc: doc.startswith(\"train\"),documents));\n",
    "print(str(len(docs)) + \" total train documents\");\n",
    "\n",
    "#To access the content of a news article, we can use the reuters.words() function\n",
    "print(\"The first document contains \"+str(len(reuters.words(docs[0])))+\" words.\\nHere they are:\")\n",
    "# for word in reuters.words(docs[0]):\n",
    "    # print(word)\n",
    "\n",
    "#Estimate inverse document frequencies based on a corpus of documents.\n",
    "def IDF(corpus):\n",
    "    idfs = {}\n",
    "    all_vec = {}\n",
    "    vecs = []\n",
    "    for text in corpus:\n",
    "        words = reuters.words(text)\n",
    "        bow = {}\n",
    "        for word in words:\n",
    "            word = word.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "            if word in bow:\n",
    "                bow[word] += 1\n",
    "            else:\n",
    "                bow[word] = 1\n",
    "        vecs.append(bow)\n",
    "        \n",
    "    for vec in vecs:\n",
    "        for word in vec:\n",
    "            if word in all_vec:\n",
    "                all_vec[word] += 1\n",
    "            else:\n",
    "                all_vec[word] = 1\n",
    "                \n",
    "    for text in corpus:\n",
    "        words = reuters.words(text)\n",
    "        for word in words:\n",
    "            word = word.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "            if word not in idfs:\n",
    "                idfs[word] = math.log10(len(corpus) / all_vec[word])\n",
    "    print(idfs)\n",
    "    return idfs\n",
    "\n",
    "#Turn a corpus of arbitrary texts into TF-IDF weighted BOW vectors.\n",
    "def TFIDF(corpus):\n",
    "    vecs = []\n",
    "    # First get a frequency-weighted bag of words for every text in fwbogs\n",
    "    fwbogs = []\n",
    "    for text in corpus:\n",
    "        words = reuters.words(text)\n",
    "        fwbogs.append(fwbog(words))\n",
    "\n",
    "    # Secondly calculate the TF-IDF\n",
    "    for i, text in enumerate(corpus):\n",
    "        tf_idf_vec = {}\n",
    "        words = reuters.words(text)\n",
    "\n",
    "        # Count frequency in all documents\n",
    "        for word in words:\n",
    "            word = word.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "            document_count = 0\n",
    "            for fwbog in fwbogs:\n",
    "                if word in fwbog:\n",
    "                    document_count += 1\n",
    "            # Add word to tf_idf_vec\n",
    "            word_fwbogs = fwbogs[i]\n",
    "            print(word_fwbogs)\n",
    "            tf_idf_vec[word] = word_fwbogs[word] * math.log10(len(corpus) / document_count)\n",
    "        vecs.append(tf_idf_vec)\n",
    "    print(vecs)\n",
    "\n",
    "def fwbog(words):\n",
    "    bow = {}\n",
    "    for word in words:\n",
    "        word = word.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "        if word in bow:\n",
    "            bow[word] += 1\n",
    "        else:\n",
    "            bow[word] = 1\n",
    "    return bow\n",
    "\n",
    "TFIDF(docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Bag-of-words models are order invariant. They do not retain the ordering in which terms occur in the document. Is there any way to include term order information in these models? Justify your answer below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Topic Models\n",
    "Topic models represent textual documents in terms of their distribution of latent topics. Imagine you have trained a 10-topic LDA model. Each topic is a frequency distribution over thousands of terms. Is there a good way of illustrating the meaning of the learned topics to a human? Discuss the advantages and disadvantages of some of the possible options below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
