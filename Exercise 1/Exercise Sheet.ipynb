{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 4271 - Exercise 1 - Web Crawling\n",
    "\n",
    "Issued: April 16, 2024\n",
    "\n",
    "Due: April 22, 2024\n",
    "\n",
    "Please submit this filled sheet via Ilias by the due date.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Duplicate Detection\n",
    "When crawling large numbers of Web pages we are likely to encounter a considerable number of duplicate documents. To not flood our index with replicas of the same documents, we need a duplicate detection scheme.\n",
    "\n",
    "a) Using python's built-in hash() function, process the following documents in order of appearance and flag up any exact duplicates.\n",
    "\n",
    "- **D1** \"This is just some document\"\n",
    "- **D2** \"This is another piece of text\"\n",
    "- **D3** \"This is another piece of text\"\n",
    "- **D4** \"This is just some documents\"\n",
    "- **D5** \"Totally different stuff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check a single document against an existing collection of previsouly seen documents for exact duplicates.\n",
    "def check_exct(doc, docs):\n",
    "    docHash = hash(doc[1])\n",
    "    for x in docs:\n",
    "        if hash(x[1]) == docHash:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Going beyond exact duplicates, we want to also identify any near-duplicates that are very similar but not identical to previously seen content. Implement the SimHash method discussed in class and again process the five documents, this time flagging up exact and near duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "NUM_HASH_BITS = 128\n",
    "\n",
    "#Check a single document against an existing collection of previsouly seen documents for near duplicates\n",
    "def check_simhash(doc, docs):\n",
    "    checkFingerprint = computeFingerprint(doc)\n",
    "    for x in docs:\n",
    "        difference = calculateDifference(checkFingerprint, computeFingerprint(x))\n",
    "        if difference < 7:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def computeFingerprint(doc):\n",
    "    weightedWords = weight(doc[1])\n",
    "    addedColumns = [0] * NUM_HASH_BITS\n",
    "    for word in weightedWords:\n",
    "        hashFnk = hashlib.md5()\n",
    "        hashFnk.update(word.encode('utf-8'))\n",
    "        hash = hashFnk.hexdigest()\n",
    "        binaryWord = str(int(hash, 16))\n",
    "        for y in range(len(binaryWord)):\n",
    "            if binaryWord[y] == \"0\":\n",
    "                addedColumns[y] = -1 * weightedWords[word]\n",
    "            else:\n",
    "                addedColumns[y] = int(binaryWord[y]) * weightedWords[word]\n",
    "    fingerprint = \"\"\n",
    "    for x in addedColumns:\n",
    "        if x > 0:\n",
    "            fingerprint += \"1\"\n",
    "        else:\n",
    "            fingerprint += \"0\"\n",
    "    return fingerprint\n",
    "\n",
    "def calculateDifference(hash1, hash2):\n",
    "    return sum(bit1 != bit2 for bit1, bit2 in zip(hash1, hash2))\n",
    "\n",
    "def weight(doc):\n",
    "    weightedWords = {}\n",
    "    arr = doc.split()\n",
    "    for word in arr:\n",
    "        if word in weightedWords:\n",
    "            weightedWords[word] = weightedWords[word] + 1\n",
    "        else:\n",
    "            weightedWords[word] = 1\n",
    "    return weightedWords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DUPLICATE: D3\n",
      "DUPLICATE: D4\n"
     ]
    }
   ],
   "source": [
    "crawl = [['D1', 'This is just some document'], ['D2', 'This is another piece of text'], ['D3', 'This is another piece of text'], ['D4', 'This is just some documents'], ['D5', 'Totally different stuff']]\n",
    "\n",
    "#Process raw crawled website content\n",
    "def process(crawl):\n",
    "    docs = []\n",
    "    for doc in crawl:\n",
    "        if check_simhash(doc, docs): #Can be exchanged for check_simhash()\n",
    "            print('DUPLICATE: '+doc[0])\n",
    "        else:\n",
    "            docs.append(doc)\n",
    "\n",
    "process(crawl)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Focused Search Engines\n",
    "Suppose you were to build a COVID-19 Web search engine for which you want to collect and eventually serve only COVID-19 information. The general web crawling process follows this scheme:\n",
    "\n",
    "1. Create a seed set of known URLs (a.k.a the frontier)\n",
    "2. Pull a URL from the frontier and visit it\n",
    "3. Save the page content for our search engine (indexing)\n",
    "4. Once on the page, note down all URLs linked there\n",
    "5. Put all encountered URLs in the queue\n",
    "6. Repeat from Step 2 until the queue is empty\n",
    "\n",
    "In this particular setting, how should the generic step-by-step crawling process be modified/extended? Discuss all relevant considerations:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The crawling process need to be extended by the following steps:\n",
    "- In step 1 we should put URLs into the frontier, that are related to COVID-19, starting with the most relevant ones.\n",
    "- In step 4 the linked URLs should only be put down if the content of the page or the URL itself is indicating that the URLs are relevant to COVID-19 in any way. This could be done by checking for existing COVID-19 related keywords.\n",
    "- In step 5 the encountered URLs should be put in the queue by prioritizing URLs based on \n",
    "    - Relevancy to COVID-19\n",
    "    - Freshness of the website (Prioritize if there was recent change)\n",
    "    - Quality of the source (to get more reliable information)\n",
    "    while we also want to filter duplicates/near duplicates and don't put them in the queue.\n",
    "- In step 3 we could also add another layer of checking if the content we are saving for indexing is really COVID-19 related and discard the page URL otherwise.\n",
    "- We should also recrawl sites that are frequently changing like news sites or governmental information/warning sites more often."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
